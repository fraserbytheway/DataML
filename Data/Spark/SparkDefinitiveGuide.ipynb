{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Spark Definitive Guide Textbook Notes",
   "id": "9036350e848d5c2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Chapter 1 - Overview of Big Data and Spark",
   "id": "f20a528504da6a35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Apache Spark - unified computing engine and set of libraries for parallel data processing on computer clusters\n",
    "* designed to support a wide range of data analytics tasks, over same computing engine with consistent API's\n",
    "* Especially helpful for big data, where local computational resources are insufficient\n",
    "\n",
    "Scope - computing engine. Focuses on performing computations over data, no matter where it resides.\n",
    "\n",
    "Motivation - Collecting data is extremely inexpensive, but processing it requires large, parallel computations, often on cluster machines.\n",
    "\n",
    "Spark differentiator - focuses on just the computation, trying to be flexible with the sources of data it can work on"
   ],
   "id": "6b9443540e3bb260"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Chapter 2 - A gentle introduction to Spark",
   "id": "7f5a2ef0698afa21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Basic Architecture\n",
    "\n",
    "Single machines do not have enough power and resources to perform computations on large amounts of information. A **cluster** pools the resources of many machines together, giving us the ability to use all the cumulative resources as if they were a single computer. Spark manages and coordinates the execution of tasks on data across clusters of computers\n",
    "\n",
    "##### Spark Applications\n",
    "\n",
    "Cluster manager:\n",
    "* keeps track of resources available\n",
    "\n",
    "driver:\n",
    "* maintains info about spark application\n",
    "* responds to user's program/input\n",
    "* analyse, distribute and schedule work across executors\n",
    "\n",
    "executors:\n",
    "* responsible for actually carrying out the work\n",
    "* execute code assigned to it, report the state of computation\n",
    "\n",
    "##### Language API's\n",
    "\n",
    "make it possible to run spark code using various programming languages. At core, executors run on the JVM"
   ],
   "id": "bb8d24e635ecc6b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T03:59:03.053991Z",
     "start_time": "2026-02-10T03:59:03.045392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import findspark\n",
    "from tensorboard.compat.tensorflow_stub.tensor_shape import vector\n",
    "\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n"
   ],
   "id": "ad102cd7db3524c9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Dataframe\n",
    "\n",
    "The most common structured API, simply represents a table of data. Similar to a spreadsheet, though can span thousands of computers\n",
    "\n",
    "Partitions - chunks of the spreadsheet on each computer. Parallelism limited by number of partitions\n",
    "\n",
    "Core data structure is immutable, to change, need to use transformations"
   ],
   "id": "ed40013b41ff608c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T03:42:40.828243Z",
     "start_time": "2026-02-10T03:42:39.947636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "myRange = spark.range(1000).toDF('number')\n",
    "myRange.show(5)"
   ],
   "id": "18dd627777e0f930",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T03:47:04.234479Z",
     "start_time": "2026-02-10T03:47:03.966221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example transformation\n",
    "\n",
    "divisby2 = myRange.where('number % 2 == 0')\n",
    "divisby2.show(5)"
   ],
   "id": "893235b79f750bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Transformation types\n",
    "\n",
    "Narrow transformation - each input partition will contribute to only one output partition\n",
    "\n",
    "Wide transformation - input partitions contribute to many output partitions. Spark exchanges partitions across the cluster\n",
    "\n",
    "##### Lazy evaluation\n",
    "\n",
    "Spark waits till very last moment to execute graph of computation instructions. Means spark can optimise the entire data workflow from end to end.\n",
    "\n",
    "Actions - used to trigger the computation"
   ],
   "id": "eb244c7d4511759c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:11:59.183570Z",
     "start_time": "2026-02-10T04:11:59.087397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# using schema inference, so Spark takes a best guess at what schema of dataframe should be\n",
    "\n",
    "# reading data a lazy operation\n",
    "\n",
    "flightData2015 = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"data/flight-data/csv/2015-summary.csv\")"
   ],
   "id": "bece7e2457e13b0a",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:03:49.399249Z",
     "start_time": "2026-02-10T04:03:49.360409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add sort, and view the lazy computation plan\n",
    "flightData2015.sort(\"count\").explain()"
   ],
   "id": "c690f301b93ff384",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#31 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#31 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=61]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#29,ORIGIN_COUNTRY_NAME#30,count#31] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/fraserbytheway/PycharmProjects/DataML/Data/Spark/data/flig..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:07:24.946438Z",
     "start_time": "2026-02-10T04:07:24.916376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# specify number of data partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "flightData2015.sort(\"count\").take(2)"
   ],
   "id": "9aeabb3219c3dd78",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:07:55.532955Z",
     "start_time": "2026-02-10T04:07:55.517715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Change dataframe into a table or view\n",
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ],
   "id": "7f9c7a4f124a3ef4",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:09:37.945046Z",
     "start_time": "2026-02-10T04:09:37.850636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SQL Query\n",
    "\n",
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "\n",
    "dataFrameWay = flightData2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()"
   ],
   "id": "2e6fe7e9906828fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#29], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#29, 5), ENSURE_REQUIREMENTS, [plan_id=182]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#29], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#29] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/fraserbytheway/PycharmProjects/DataML/Data/Spark/data/flig..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#29], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#29, 5), ENSURE_REQUIREMENTS, [plan_id=195]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#29], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#29] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/fraserbytheway/PycharmProjects/DataML/Data/Spark/data/flig..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:12:54.769882Z",
     "start_time": "2026-02-10T04:12:54.595240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# find max (effectively filtering down to one row)\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "flightData2015.select(max('count')).take(1)"
   ],
   "id": "5384f63ebdf31dca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exectution plan is a directed acyclic graph (DAG) of transformations",
   "id": "11367583ad6fa927"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:17:30.699738Z",
     "start_time": "2026-02-10T04:17:30.610802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# more complicated expression\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .explain()\n",
    "\n",
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ],
   "id": "7d9af4ff443fc0f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#162L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#115,destination_total#162L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#115], functions=[sum(count#117)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#115, 5), ENSURE_REQUIREMENTS, [plan_id=309]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#115], functions=[partial_sum(count#117)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#115,count#117] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/fraserbytheway/PycharmProjects/DataML/Data/Spark/data/flig..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n",
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Chapter 3 - A Tour of Spark's Toolset",
   "id": "a98af4f0d2339d25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Datasets - type-safe structured APIs\n",
    "\n",
    "Used for writing statically typed code in Java and Scala. Cannot accidently view the objects in a dataset as being of another class, than the class put in initially.\n",
    "\n",
    "* Dataset[Person] will be guaranteed to contain objects of type person\n",
    "\n",
    "##### Structured Streaming\n",
    "\n",
    "Take same operations performed in batch mode using Spark's structured API, and run them in a streaming fashion. This can reduce latency and allow for incremental processing.\n",
    "\n",
    "With streaming data keeps arriving, treats incoming data as rows being appended to an \"Unbounded Table.\""
   ],
   "id": "9e48292928a00fcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:29:10.129569Z",
     "start_time": "2026-02-10T04:29:08.682897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# structured streaming\n",
    "\n",
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"data/retail-data/by-day/*.csv\")\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ],
   "id": "a192e9e0235a78cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/10 15:29:08 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: data/retail-data/by-day/*.csv.\n",
      "java.io.FileNotFoundException: File data/retail-data/by-day/*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:980)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1301)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:970)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.sinks.FileStreamSink$.hasMetadata(FileStreamSink.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:384)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:99)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:32:54.827621Z",
     "start_time": "2026-02-10T04:32:54.446720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# some data grouping operations\n",
    "\n",
    "from pyspark.sql.functions import window, column, desc, col\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "staticDataFrame\\\n",
    "    .selectExpr(\n",
    "    \"CustomerID\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "    .groupBy(\n",
    "    col(\"CustomerID\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "    .sum(\"total_cost\")\\\n",
    "    .show(5)"
   ],
   "id": "d6eeee6705fb9cf1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   13417.0|{2011-12-04 11:00...|            404.83|\n",
      "|   12782.0|{2011-12-04 11:00...|252.24999999999997|\n",
      "|   16513.0|{2011-12-04 11:00...|             121.8|\n",
      "|   15392.0|{2011-12-05 11:00...|304.40999999999997|\n",
      "|   15290.0|{2011-12-05 11:00...|263.02000000000004|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:34:05.914363Z",
     "start_time": "2026-02-10T04:34:05.869617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# using streaming instead\n",
    "streamingDataFrame = spark.readStream\\\n",
    ".schema(staticSchema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(\"/data/retail-data/by-day/*.csv\")"
   ],
   "id": "81f414bd3fec8768",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:34:21.761941Z",
     "start_time": "2026-02-10T04:34:21.754378Z"
    }
   },
   "cell_type": "code",
   "source": "streamingDataFrame.isStreaming",
   "id": "396a008f3e82666",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:34:42.993781Z",
     "start_time": "2026-02-10T04:34:42.965525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    ".selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(UnitPrice * Quantity) as total_cost\",\n",
    "\"InvoiceDate\")\\\n",
    ".groupBy(\n",
    "col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")"
   ],
   "id": "9129cd054a2d9e19",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "triggering the streaming operation. Bit different as result is being populating data somewhere",
   "id": "4122972c45f6a946"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:58:40.967518Z",
     "start_time": "2026-02-10T04:58:40.867880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"memory\")\\\n",
    ".queryName(\"customer_purchases\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ],
   "id": "ae423d9963a09008",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/10 15:58:40 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/65/kfyt6zd17rnft11mhd0p3ysc0000gn/T/temporary-f6165344-8dd3-42ce-ac71-221d32607ae5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "26/02/10 15:58:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name customer_purchases as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIllegalArgumentException\u001B[39m                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[91]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[43mpurchaseByCustomerPerHour\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwriteStream\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m.\u001B[49m\u001B[43mqueryName\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcustomer_purchases\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m.\u001B[49m\u001B[43moutputMode\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcomplete\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DataML/.venv/lib/python3.11/site-packages/pyspark/sql/streaming/readwriter.py:1704\u001B[39m, in \u001B[36mDataStreamWriter.start\u001B[39m\u001B[34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[39m\n\u001B[32m   1702\u001B[39m     \u001B[38;5;28mself\u001B[39m.queryName(queryName)\n\u001B[32m   1703\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1704\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sq(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jwrite\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   1705\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1706\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sq(\u001B[38;5;28mself\u001B[39m._jwrite.start(path))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DataML/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1356\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1357\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1358\u001B[39m     args_command +\\\n\u001B[32m   1359\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1361\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DataML/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:269\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    265\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    266\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    267\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    268\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m269\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    270\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    271\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mIllegalArgumentException\u001B[39m: Cannot start query with name customer_purchases as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:39:41.103227Z",
     "start_time": "2026-02-10T04:39:41.044057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\")\\\n",
    ".show(5)"
   ],
   "id": "c86f9fa8b7aee435",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+\n",
      "|CustomerId|window|sum(total_cost)|\n",
      "+----------+------+---------------+\n",
      "+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:39:43.187051Z",
     "start_time": "2026-02-10T04:39:43.081237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"console\")\\\n",
    ".queryName(\"customer_purchases_2\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ],
   "id": "9970e0917159b69d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/10 15:39:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/65/kfyt6zd17rnft11mhd0p3ysc0000gn/T/temporary-5654c80b-46df-405c-b3cf-213552bb2f3d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "26/02/10 15:39:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name customer_purchases_2 as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIllegalArgumentException\u001B[39m                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[70]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[43mpurchaseByCustomerPerHour\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwriteStream\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mconsole\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m.\u001B[49m\u001B[43mqueryName\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcustomer_purchases_2\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m.\u001B[49m\u001B[43moutputMode\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcomplete\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DataML/.venv/lib/python3.11/site-packages/pyspark/sql/streaming/readwriter.py:1704\u001B[39m, in \u001B[36mDataStreamWriter.start\u001B[39m\u001B[34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[39m\n\u001B[32m   1702\u001B[39m     \u001B[38;5;28mself\u001B[39m.queryName(queryName)\n\u001B[32m   1703\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1704\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sq(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jwrite\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   1705\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1706\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sq(\u001B[38;5;28mself\u001B[39m._jwrite.start(path))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DataML/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1356\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1357\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1358\u001B[39m     args_command +\\\n\u001B[32m   1359\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1361\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DataML/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:269\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    265\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    266\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    267\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    268\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m269\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    270\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    271\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mIllegalArgumentException\u001B[39m: Cannot start query with name customer_purchases_2 as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Machine Learning and Advanced Analytics\n",
    "\n",
    "Can perform large scale machine learning with a built - in library of machine learning algorithms, called MLlib."
   ],
   "id": "4d4b62b9fa5aabbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:41:15.230039Z",
     "start_time": "2026-02-10T04:41:15.220237Z"
    }
   },
   "cell_type": "code",
   "source": "staticDataFrame.printSchema()",
   "id": "34c6bd9c67377e96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:42:40.119638Z",
     "start_time": "2026-02-10T04:42:40.076917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "preppedDataFrame = staticDataFrame\\\n",
    "    .na.fill(0)\\\n",
    "    .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
    "    .coalesce(5) # used to efficiently reduce the number of partitions"
   ],
   "id": "b5bb64700e127f96",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:44:51.554790Z",
     "start_time": "2026-02-10T04:44:51.517864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split into train and test data\n",
    "trainDataFrame = preppedDataFrame\\\n",
    "    .where(\"InvoiceDate < '2011-07-01'\")\n",
    "\n",
    "testDataFrame = preppedDataFrame\\\n",
    "    .where(\"InvoiceDate >= '2011-07-01'\")"
   ],
   "id": "3987f77dbccd59ea",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:45:37.216553Z",
     "start_time": "2026-02-10T04:45:36.934509Z"
    }
   },
   "cell_type": "code",
   "source": "trainDataFrame.count()",
   "id": "fde6d28d35956615",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245903"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:45:41.204114Z",
     "start_time": "2026-02-10T04:45:40.922988Z"
    }
   },
   "cell_type": "code",
   "source": "testDataFrame.count()",
   "id": "82058b9dec4bee1b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296006"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transforming the day of the week into a OHE numeric feature",
   "id": "21d41bf76dd96b20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:46:07.295020Z",
     "start_time": "2026-02-10T04:46:07.044451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "indexer = StringIndexer()\\\n",
    ".setInputCol(\"day_of_week\")\\\n",
    ".setOutputCol(\"day_of_week_index\")"
   ],
   "id": "b41db46ad63bc5c0",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:46:34.442863Z",
     "start_time": "2026-02-10T04:46:34.428589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\\\n",
    ".setInputCol(\"day_of_week_index\")\\\n",
    ".setOutputCol(\"day_of_week_encoded\")"
   ],
   "id": "588c593d4349364f",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creating input vector, a set of numeric columns",
   "id": "7c50ca99cc4c866f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:48:41.411662Z",
     "start_time": "2026-02-10T04:48:41.402379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "    .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "    .setOutputCol(\"features\")"
   ],
   "id": "ba361d47c1766131",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pipeline to make transformations reproducible",
   "id": "62712f815acd2600"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:49:33.545646Z",
     "start_time": "2026-02-10T04:49:33.540674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "    .setStages([indexer, encoder, vectorAssembler])"
   ],
   "id": "94143360e34cd054",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:50:01.007473Z",
     "start_time": "2026-02-10T04:49:59.566810Z"
    }
   },
   "cell_type": "code",
   "source": "fittedPipeline = transformationPipeline.fit(trainDataFrame)",
   "id": "be6f09cb4bb3248f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:50:30.257415Z",
     "start_time": "2026-02-10T04:50:30.200683Z"
    }
   },
   "cell_type": "code",
   "source": "transformedTraining = fittedPipeline.transform(trainDataFrame)",
   "id": "a6dfbe3617f6ded1",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Cache\n",
    "\n",
    "With cache - spark computes the dataframe once, saves it to memory, and resuses it for subsequent actions. Means spark doesn't need to recompute the entire dataframe"
   ],
   "id": "8ced3f2fa36147a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "transformedTraining.cache()",
   "id": "c218235b8cc3710c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:52:36.154082Z",
     "start_time": "2026-02-10T04:52:36.131207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans()\\\n",
    ".setK(20)\\\n",
    ".setSeed(0)"
   ],
   "id": "dacd9ad03b8a47df",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Training Machine Learning Models\n",
    "\n",
    "two-phase process. First, initialise an untrained model, then train it. Follow the naming pattern of Algorithm for untrained, AlgorithmModel for trained version."
   ],
   "id": "7a7c6d1e0b970910"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:54:16.257733Z",
     "start_time": "2026-02-10T04:54:07.915051Z"
    }
   },
   "cell_type": "code",
   "source": "kmModel = kmeans.fit(transformedTraining)",
   "id": "e3fdff163e67a6a6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/10 15:54:15 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T04:55:06.480857Z",
     "start_time": "2026-02-10T04:55:06.444317Z"
    }
   },
   "cell_type": "code",
   "source": "transformedTest = fittedPipeline.transform(testDataFrame)",
   "id": "eaa313aa013f6e0a",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Lower-Level APIs\n",
    "\n",
    "Spark has a variety of lower level primitives to allow for arbritary Java and Python object manipulation via RDDs.\n",
    "\n",
    "Resilient Distributed Datasets (RDDs)\n",
    "* reveal physical execution characteristics\n",
    "* can be used to parallelise raw data\n"
   ],
   "id": "107ac6ffbe81103"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Chapter 4 - Structured API's",
   "id": "dd7711a96fa3a079"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tool for manipulating all sorts of data. These APIs refer to three core types of distributed collection APIs:\n",
    "* Datasets\n",
    "* Dataframes\n",
    "* SQL tables and views\n",
    "\n",
    "Spark recap: Spark is a distributed programming model in which the user specifies transformations. Multiple transformations build up a directed acyclic graph of instructions. An action begins the process of executing these instructions, as a single job, by breaking it down into stages and clusters. The logical structures that we manipulate with transformations and actions are Dataframes and Datasets\n",
    "\n",
    "Dataframes/sets are distributed table like collections with well defined rows and columns. Properties:\n",
    "\n",
    "* schemas - defines column names and types of a dataframe. Can be defined or read from a datasource\n",
    "\n",
    "##### Spark Types\n",
    "\n",
    "Spark is effectively a programming language of its own. Uses an engine called catalyst that maintains its own type information. Spark will convert an expression written in input to Sparks internal catalyst rep.\n",
    "\n",
    "##### Dataframes vs Datasets\n",
    "\n",
    "Dataframe - only checks whether types line up to those specified in the schema at runtime\n",
    "Dataset - checks at compiile time. Only available to JVM languages\n",
    "\n",
    "##### Columns\n",
    "\n",
    "represent a simple type like an integer or a string, a complex type like an array or map, or a null value. Spark tracks all this information, and offers a variety of ways, with which you can transform columns.\n",
    "\n",
    "##### Rows\n",
    "\n",
    "A record of data\n",
    "\n",
    "##### Types - See page 60 for useful manual"
   ],
   "id": "e85f9f5f6d21ba93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Structured API Execution\n",
    "\n",
    "1. Write dataframe code\n",
    "2. If valid, spark converts to a logical plan\n",
    "3. Spark transforms logical plan to a physical plan, checking optimisations along the way\n",
    "4. Spark then executes the physical plan (RDD manipulations) on the cluster\n",
    "\n",
    "\n",
    "##### Logical Plan\n",
    "Only represents a set of abstract transformations that do not refer to executors or drivers. Purely to convert the user's set of expressions into the most optimised version.\n",
    "\n",
    "Spark uses the catalog, a repository of all table and DataFrame information, to resolve\n",
    "columns and tables in the analyzer. The analyzer might reject the unresolved logical plan if the\n",
    "required table or column name does not exist in the catalog. If the analyzer can resolve it, the\n",
    "result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the\n",
    "logical plan by pushing down predicates or selections.\n",
    "\n",
    "##### Physical Plan\n",
    "\n",
    "After successfully creating an optimised logical plan, spark begins physical planning process. Specifies how plan will execute on the cluster, by generating different physical execution strategies, and comparing them through a cost model.\n",
    "\n",
    "##### Execution\n",
    "\n",
    "Once spark has a physical plan, proceeds to execute the code over RDDs"
   ],
   "id": "e6a1d983c1d9d106"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Chapter 5 - Basic Structured Operations",
   "id": "ecbc1518d6094040"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Basic Structure\n",
    "\n",
    "Dataframe consists of a series of records (like rows in a table), and columns (like spreadsheet columns) that represent a computation expression that can be performed on each individual record in the dataset. Schemas define name and type of data in each column. Partitioning defines layout of DataFrame across the cluster"
   ],
   "id": "9405a803d2b772d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:27:38.041710Z",
     "start_time": "2026-02-10T05:27:37.828831Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.format(\"json\").load(\"data/flight-data/json/2015-summary.json\")",
   "id": "63788dd3087311d0",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:27:48.833592Z",
     "start_time": "2026-02-10T05:27:48.825561Z"
    }
   },
   "cell_type": "code",
   "source": "df.printSchema()",
   "id": "a8a2d10dea086685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Can either let a data source define the schema, or define it explicitly ourselves. Schema inferene is less safe, as sometimes types are inferred. Schema format below",
   "id": "5374a8398582d45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:29:18.394655Z",
     "start_time": "2026-02-10T05:29:18.330595Z"
    }
   },
   "cell_type": "code",
   "source": "spark.read.format(\"json\").load(\"data/flight-data/json/2015-summary.json\").schema",
   "id": "a1ec599efa295036",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Schema is a structtype, made up of a number of fields, StructFields. These have a name, type, a Boolean flag for missing values or null values, and user can optionally specify associated metadata with that column",
   "id": "1662fd9e70c5ceae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:34:01.066806Z",
     "start_time": "2026-02-10T05:34:01.050488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Manual Schema Specification\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    ".load(\"data/flight-data/json/2015-summary.json\")"
   ],
   "id": "c94fb014d35f9700",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Columns and Expressions\n",
    "\n",
    "Columns are logical constructions that simply represent a value computed on a per - record basis, by means of an expression"
   ],
   "id": "8be1621f09cb14d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:36:00.525340Z",
     "start_time": "2026-02-10T05:36:00.479460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ways to construct and refer to a column:\n",
    "from pyspark.sql.functions import col, column\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")"
   ],
   "id": "7dc83cbe77aceaf0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'someColumnName'>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Expressions - set of transformations on one or more values in a record in a Dataframe. Main thing is columns are just an expression, in samplist case is just like x = x",
   "id": "e7b095275584e3dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:39:08.597911Z",
     "start_time": "2026-02-10T05:39:08.508391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# access columns\n",
    "spark.read.format(\"json\").load(\"data/flight-data/json/2015-summary.json\")\\\n",
    ".columns"
   ],
   "id": "4de53a02e1180ca1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Records and Rows\n",
    "\n",
    "Each row in a dataframe is a single record. Spark represents this as an object of type row. Spark manipulates row objects using column expressions in order to produce usable values"
   ],
   "id": "98e60a5b83946a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:41:55.415070Z",
     "start_time": "2026-02-10T05:41:55.361035Z"
    }
   },
   "cell_type": "code",
   "source": "df.first()",
   "id": "91370c0430559af8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:43:27.500255Z",
     "start_time": "2026-02-10T05:43:27.495413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)\n",
    "myRow[0]"
   ],
   "id": "b1890aec2017aef5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Dataframe Transformations\n",
    "\n"
   ],
   "id": "74231247aa1cd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:45:04.400375Z",
     "start_time": "2026-02-10T05:45:03.536431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a dataframe\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"some\", StringType(), True),\n",
    "StructField(\"col\", StringType(), True),\n",
    "StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ],
   "id": "fb5c453675aec7ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|NULL|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Select method, select columns within the dataframe",
   "id": "9e0123e221b5a110"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:46:05.045925Z",
     "start_time": "2026-02-10T05:46:04.994895Z"
    }
   },
   "cell_type": "code",
   "source": "df.select(\"DEST_COUNTRY_NAME\").show(2)",
   "id": "7d24333a99ca074",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:47:26.368132Z",
     "start_time": "2026-02-10T05:47:26.288877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "# can refer to columns in a number of different ways\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    "expr(\"DEST_COUNTRY_NAME\"),\n",
    "col(\"DEST_COUNTRY_NAME\"),\n",
    "column(\"DEST_COUNTRY_NAME\"))\\\n",
    ".show(2)"
   ],
   "id": "a024639f77e265ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:49:12.308021Z",
     "start_time": "2026-02-10T05:49:12.231079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using select expressions to build up more complex dataframes\n",
    "\n",
    "df.selectExpr(\n",
    "\"*\", # all original columns\n",
    "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    ".show(2)"
   ],
   "id": "719d73201d28ddf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:49:35.797819Z",
     "start_time": "2026-02-10T05:49:35.629829Z"
    }
   },
   "cell_type": "code",
   "source": "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)",
   "id": "6fa808252d12c78d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using literals",
   "id": "174de6804097b871"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:50:39.334123Z",
     "start_time": "2026-02-10T05:50:39.229713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ],
   "id": "b826cca7747bd1b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Will come up when need to check whether a value is greater than some constant, or other programmatically created variable**",
   "id": "62578e5bf8dff816"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# alternate method, using with column",
   "id": "f7cfe31bb36b53b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T05:51:43.641952Z",
     "start_time": "2026-02-10T05:51:43.565262Z"
    }
   },
   "cell_type": "code",
   "source": "df.withColumn(\"numberOne\", lit(1)).show(2)",
   "id": "797ec0bd7054b908",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ".withColumnRenamed to rename columns\n",
    "\n",
    "by default spark is case insensitive, though can make case sensitive by setting the config caseSensitive to Trye\n",
    "\n",
    "can use df.drop(\"col\") to drop specific columns"
   ],
   "id": "8c0aa8a67e8b91ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Up to page 80",
   "id": "2c04329d4a4ef794"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
